import { GoogleGenerativeAI } from '@google/generative-ai'
import { API_CONFIG } from './api.js'
import { getSystemPrompt } from './company-context.js'

// Láº¥y API key tá»« config hoáº·c environment variable
const getApiKey = () => {
  // Æ¯u tiÃªn config trÆ°á»›c, sau Ä‘Ã³ má»›i Ä‘áº¿n environment variable
  if (API_CONFIG.GEMINI_API_KEY && API_CONFIG.GEMINI_API_KEY !== 'your_gemini_api_key_here') {
    return API_CONFIG.GEMINI_API_KEY
  }
  // Fallback vá» environment variable
  if (import.meta.env.VITE_GEMINI_API_KEY) {
    return import.meta.env.VITE_GEMINI_API_KEY
  }
  return null
}

// Khá»Ÿi táº¡o Google Generative AI
const genAI = new GoogleGenerativeAI(getApiKey())

// Táº¡o model Gemini 2.5 Flash (model má»›i nháº¥t, tá»‘t nháº¥t vá» giÃ¡/hiá»‡u suáº¥t)
export const geminiModel = genAI.getGenerativeModel({ model: 'gemini-2.5-flash' })

// HÃ m delay
const delay = (ms) => new Promise(resolve => setTimeout(resolve, ms))

// HÃ m retry vá»›i exponential backoff
const retryWithBackoff = async (fn, maxRetries = 3, baseDelay = 2000) => {
  for (let i = 0; i < maxRetries; i++) {
    try {
      return await fn()
    } catch (error) {
      // Kiá»ƒm tra náº¿u lÃ  lá»—i 429 (Too Many Requests)
      const is429Error = error.message?.includes('429') ||
        error.message?.includes('Too Many Requests') ||
        error.message?.includes('RESOURCE_EXHAUSTED') ||
        error.status === 429

      // Náº¿u lÃ  lá»—i 429 vÃ  cÃ²n retry, thÃ¬ retry
      if (is429Error && i < maxRetries - 1) {
        const waitTime = baseDelay * Math.pow(2, i) // Exponential backoff: 2s, 4s, 8s
        console.log(`â³ Lá»—i 429 - Äá»£i ${waitTime / 1000}s trÆ°á»›c khi thá»­ láº¡i (láº§n ${i + 1}/${maxRetries})...`)
        await delay(waitTime)
        continue
      }

      // Náº¿u khÃ´ng pháº£i lá»—i 429 hoáº·c háº¿t retry, throw error
      throw error
    }
  }
}

// HÃ m gá»i API Gemini
export const generateGeminiResponse = async (prompt, conversationHistory = []) => {
  try {
    // Kiá»ƒm tra API key
    const apiKey = getApiKey()
    if (!apiKey) {
      throw new Error('Vui lÃ²ng cáº¥u hÃ¬nh API key Gemini trong file config/api.js')
    }

    // Táº¡o system prompt vá»›i context cÃ´ng ty
    const systemPrompt = getSystemPrompt()

    // HÃ m gá»i API (sáº½ Ä‘Æ°á»£c retry náº¿u gáº·p lá»—i 429)
    const callApi = async () => {
      // Táº¡o chat session vá»›i system prompt
      const chat = geminiModel.startChat({
        history: [
          {
            role: 'user',
            parts: [{ text: systemPrompt }]
          },
          {
            role: 'model',
            parts: [{ text: 'TÃ´i Ä‘Ã£ hiá»ƒu rÃµ thÃ´ng tin vá» MATECOM vÃ  sáºµn sÃ ng há»— trá»£ khÃ¡ch hÃ ng. TÃ´i sáº½ tráº£ lá»i cÃ¡c cÃ¢u há»i vá» sáº£n pháº©m, dá»‹ch vá»¥ vÃ  tÆ° váº¥n gÃ³i phÃ¹ há»£p.' }]
          },
          ...conversationHistory
        ],
        generationConfig: {
          maxOutputTokens: 1500,
          temperature: 0.7,
          topP: 0.8,
          topK: 40,
        },
      })

      // Gá»­i prompt vÃ  nháº­n káº¿t quáº£
      const result = await chat.sendMessage(prompt)
      const response = await result.response

      // Log thÃ´ng tin vá» usage (náº¿u cÃ³)
      if (result.response?.usageMetadata) {
        console.log('ðŸ“Š Token usage:', {
          promptTokens: result.response.usageMetadata.promptTokenCount,
          responseTokens: result.response.usageMetadata.candidatesTokenCount,
          totalTokens: result.response.usageMetadata.totalTokenCount
        })
      }

      return response.text()
    }

    // Gá»i API vá»›i retry logic
    const responseText = await retryWithBackoff(callApi)

    console.log('âœ… API call thÃ nh cÃ´ng')
    return responseText

  } catch (error) {
    console.error('âŒ Lá»—i khi gá»i Gemini API:', error)

    // Xá»­ lÃ½ lá»—i cá»¥ thá»ƒ
    if (error.message?.includes('API key') || error.message?.includes('API_KEY_INVALID')) {
      throw new Error('Vui lÃ²ng cáº¥u hÃ¬nh API key Gemini há»£p lá»‡')
    } else if (error.message?.includes('429') || error.message?.includes('Too Many Requests') || error.message?.includes('RESOURCE_EXHAUSTED')) {
      throw new Error('âš ï¸ QuÃ¡ nhiá»u yÃªu cáº§u! Vui lÃ²ng Ä‘á»£i 1-2 phÃºt rá»“i thá»­ láº¡i. API Gemini cÃ³ giá»›i háº¡n sá»‘ request.')
    } else if (error.message?.includes('quota')) {
      throw new Error('ÄÃ£ háº¿t quota API. Vui lÃ²ng kiá»ƒm tra giá»›i háº¡n API key cá»§a báº¡n.')
    } else if (error.message?.includes('parts')) {
      throw new Error('Lá»—i format dá»¯ liá»‡u. Vui lÃ²ng thá»­ láº¡i.')
    } else if (error.message?.includes('404') || error.message?.includes('not found')) {
      throw new Error('Model khÃ´ng tá»“n táº¡i hoáº·c khÃ´ng Ä‘Æ°á»£c há»— trá»£. Vui lÃ²ng kiá»ƒm tra tÃªn model.')
    } else {
      throw new Error(`KhÃ´ng thá»ƒ káº¿t ná»‘i vá»›i AI: ${error.message || 'Vui lÃ²ng thá»­ láº¡i sau.'}`)
    }
  }
}

// HÃ m chuyá»ƒn Ä‘á»•i lá»‹ch sá»­ chat sang format Gemini
export const convertChatHistory = (messages) => {
  const history = []

  for (let i = 0; i < messages.length - 1; i += 2) {
    const userMessage = messages[i]
    const aiMessage = messages[i + 1]

    if (userMessage && aiMessage && userMessage.isUser && !aiMessage.isUser) {
      history.push({
        role: 'user',
        parts: [{ text: userMessage.text }]
      })
      history.push({
        role: 'model',
        parts: [{ text: aiMessage.text }]
      })
    }
  }

  return history
}
